{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmevYDp0Nmjk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrape the details of most viewed videos on YouTube from Wikipedia. Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos"
      ],
      "metadata": {
        "id": "9lR_nGCsNrWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "response = requests.get(\n",
        "\turl=\"https://en.wikipedia.org/wiki/Web_scraping\",\n",
        ")\n",
        "print(response.status_code)"
      ],
      "metadata": {
        "id": "8LRswdndNs8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python3 scraper.py\n",
        "200"
      ],
      "metadata": {
        "id": "360x7EVpRyoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install beautifulsoup4"
      ],
      "metadata": {
        "id": "p1jxLbCQR0XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "response = requests.get(\n",
        "\turl=\"https://en.wikipedia.org/wiki/Web_scraping\",\n",
        ")\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "title = soup.find(id=\"firstHeading\")\n",
        "print(title.string)"
      ],
      "metadata": {
        "id": "8OWpgJ5BR2dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "\n",
        "response = requests.get(\n",
        "\turl=\"https://en.wikipedia.org/wiki/Web_scraping\",\n",
        ")\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "title = soup.find(id=\"firstHeading\")\n",
        "print(title.content)\n",
        "\n",
        "# Get all the links\n",
        "allLinks = soup.find(id=\"bodyContent\").find_all(\"a\")\n",
        "random.shuffle(allLinks)\n",
        "linkToScrape = 0\n",
        "\n",
        "for link in allLinks:\n",
        "\t# We are only interested in other wiki articles\n",
        "\tif link['href'].find(\"/wiki/\") == -1:\n",
        "\t\tcontinue\n",
        "\n",
        "\t# Use this link to scrape\n",
        "\tlinkToScrape = link\n",
        "\tbreak\n",
        "\n",
        "print(linkToScrape)"
      ],
      "metadata": {
        "id": "6hEKUln8R5BV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "\n",
        "def scrapeWikiArticle(url):\n",
        "\tresponse = requests.get(\n",
        "\t\turl=url,\n",
        "\t)\n",
        "\n",
        "\tsoup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "\ttitle = soup.find(id=\"firstHeading\")\n",
        "\tprint(title.text)\n",
        "\n",
        "\tallLinks = soup.find(id=\"bodyContent\").find_all(\"a\")\n",
        "\trandom.shuffle(allLinks)\n",
        "\tlinkToScrape = 0\n",
        "\n",
        "\tfor link in allLinks:\n",
        "\t\t# We are only interested in other wiki articles\n",
        "\t\tif link['href'].find(\"/wiki/\") == -1:\n",
        "\t\t\tcontinue\n",
        "\n",
        "\t\t# Use this link to scrape\n",
        "\t\tlinkToScrape = link\n",
        "\t\tbreak\n",
        "\n",
        "\tscrapeWikiArticle(\"https://en.wikipedia.org\" + linkToScrape['href'])\n",
        "\n",
        "scrapeWikiArticle(\"https://en.wikipedia.org/wiki/Web_scraping\")"
      ],
      "metadata": {
        "id": "l1M6bPweSBqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrape the details team India’s international fixtures from bcci.tv.\n",
        "Url = https://www.bcci.tv/.\n",
        "You need to find following details:"
      ],
      "metadata": {
        "id": "RyEkP9POOAtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.ticker as ticker\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ],
      "metadata": {
        "id": "w6Oh7pjBORHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_pages = 2\n",
        "\n",
        "def get_data(pageNo):\n",
        "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
        "\n",
        "    r = requests.get('https://www.amazon.in/gp/bestsellers/books/ref=zg_bs_pg_'+str(pageNo)+'?ie=UTF8&pg='+str(pageNo), headers=headers)#, proxies=proxies)\n",
        "    content = r.content\n",
        "    soup = BeautifulSoup(content)\n",
        "    #print(soup)\n",
        "\n",
        "    alls = []\n",
        "    for d in soup.findAll('div', attrs={'class':'a-section a-spacing-none aok-relative'}):\n",
        "        #print(d)\n",
        "        name = d.find('span', attrs={'class':'zg-text-center-align'})\n",
        "        n = name.find_all('img', alt=True)\n",
        "        #print(n[0]['alt'])\n",
        "        author = d.find('a', attrs={'class':'a-size-small a-link-child'})\n",
        "        rating = d.find('span', attrs={'class':'a-icon-alt'})\n",
        "        users_rated = d.find('a', attrs={'class':'a-size-small a-link-normal'})\n",
        "        price = d.find('span', attrs={'class':'p13n-sc-price'})\n",
        "\n",
        "        all1=[]\n",
        "\n",
        "        if name is not None:\n",
        "            #print(n[0]['alt'])\n",
        "            all1.append(n[0]['alt'])\n",
        "        else:\n",
        "            all1.append(\"unknown-product\")\n",
        "\n",
        "        if author is not None:\n",
        "            #print(author.text)\n",
        "            all1.append(author.text)\n",
        "        elif author is None:\n",
        "            author = d.find('span', attrs={'class':'a-size-small a-color-base'})\n",
        "            if author is not None:\n",
        "                all1.append(author.text)\n",
        "            else:\n",
        "                all1.append('0')\n",
        "\n",
        "        if rating is not None:\n",
        "            #print(rating.text)\n",
        "            all1.append(rating.text)\n",
        "        else:\n",
        "            all1.append('-1')\n",
        "\n",
        "        if users_rated is not None:\n",
        "            #print(price.text)\n",
        "            all1.append(users_rated.text)\n",
        "        else:\n",
        "            all1.append('0')\n",
        "\n",
        "        if price is not None:\n",
        "            #print(price.text)\n",
        "            all1.append(price.text)\n",
        "        else:\n",
        "            all1.append('0')\n",
        "        alls.append(all1)\n",
        "    return alls"
      ],
      "metadata": {
        "id": "a_XWxNxgOSRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrape the details of State-wise GDP of India from statisticstime.com.\n",
        "Url = http://statisticstimes.com/\n",
        "You have to find following details"
      ],
      "metadata": {
        "id": "T00Xko8zOcds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = 'https://en.wikipedia.org/wiki/List_of_Indian_states_and_union_territories_by_GDP_per_capita'\n",
        "response = requests.get(url)\n",
        "content = response.content\n",
        "\n",
        "soup = BeautifulSoup(content, 'html.parser')\n",
        "table = soup.find('table', class_='wikitable')\n",
        "\n",
        "data = []\n",
        "for row in table.tbody.find_all('tr'):\n",
        "    columns = row.find_all('td')\n",
        "    if len(columns) >= 3:\n",
        "        state = columns[1].text.strip()\n",
        "        income = columns[2].text.strip().replace('₹', '').replace(',', '')\n",
        "        data.append((state, int(income)))"
      ],
      "metadata": {
        "id": "mhtPSkyVOhjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Read the shapefile\n",
        "shapefile_path = 'F:\\\\newbieron\\\\Task-3\\\\states_india\\\\states_india.shp'\n",
        "map_df = gpd.read_file(shapefile_path)\n",
        "\n",
        "# Modify the state name in the DataFrame to match the shapefile\n",
        "df['State'] = df['State'].replace('Jammu and Kashmir', 'Jammu & Kashmir')\n",
        "\n",
        "# Merge the shapefile with the per capita income data\n",
        "merged = map_df.set_index('st_nm').join(df.set_index('State'))\n",
        "\n",
        "# Set up the figure and axes\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Create the choropleth map with a custom color scheme\n",
        "merged.plot(column='Income', cmap='YlOrRd', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)\n",
        "\n",
        "# Add state names as labels\n",
        "merged['centroid'] = merged.geometry.centroid\n",
        "for x, y, label in zip(merged['centroid'].x, merged['centroid'].y, merged.index):\n",
        "    ax.text(x, y, label, fontsize=8, ha='center')\n",
        "\n",
        "\n",
        "# Set the chart title\n",
        "plt.title('Per Capita Income of Indian States')\n",
        "\n",
        "# Display the chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sWIepoUnPJSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrape the details of trending repositories on Github.com.\n",
        "Url = https://github.com/\n",
        "You have to find the following details:"
      ],
      "metadata": {
        "id": "_6KiN_1mPPzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pprint\n",
        "\n",
        "\n",
        "def get_trending_repositories():\n",
        "    url_to_call = \"https://github.com/trending\"\n",
        "\n",
        "    response = requests.get(url_to_call, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    response_code = response.status_code\n",
        "    if response_code != 200:\n",
        "        print(\"Error occurred\")\n",
        "        return\n",
        "\n",
        "    html_content = response.content\n",
        "    dom = BeautifulSoup(html_content, 'html.parser')\n",
        "    all_trending_repos = dom.select(\"article.Box-row h1\")\n",
        "\n",
        "    trending_repositories = []\n",
        "    for each_trending_repo in all_trending_repos:\n",
        "        href_link = each_trending_repo.a.attrs[\"href\"]\n",
        "        name = href_link[1:]\n",
        "        repo = {\"label\": name,\n",
        "                \"link\": \"https://github.com{}\".format(href_link)}\n",
        "        trending_repositories.append(repo)\n",
        "    return trending_repositories\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    trending_repos = get_trending_repositories()\n",
        "    pprint.pprint(trending_repos)"
      ],
      "metadata": {
        "id": "zLuXON3kPcto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IgLVRHa7Pi-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrape the details of Highest selling novels.\n",
        "compare"
      ],
      "metadata": {
        "id": "5Dq2o_WQP8Zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.ticker as ticker\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ],
      "metadata": {
        "id": "fa4sxeVgQHgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_pages = 2\n",
        "\n",
        "def get_data(pageNo):\n",
        "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
        "\n",
        "    r = requests.get('https://www.amazon.in/gp/bestsellers/books/ref=zg_bs_pg_'+str(pageNo)+'?ie=UTF8&pg='+str(pageNo), headers=headers)#, proxies=proxies)\n",
        "    content = r.content\n",
        "    soup = BeautifulSoup(content)\n",
        "    #print(soup)\n",
        "\n",
        "    alls = []\n",
        "    for d in soup.findAll('div', attrs={'class':'a-section a-spacing-none aok-relative'}):\n",
        "        #print(d)\n",
        "        name = d.find('span', attrs={'class':'zg-text-center-align'})\n",
        "        n = name.find_all('img', alt=True)\n",
        "        #print(n[0]['alt'])\n",
        "        author = d.find('a', attrs={'class':'a-size-small a-link-child'})\n",
        "        rating = d.find('span', attrs={'class':'a-icon-alt'})\n",
        "        users_rated = d.find('a', attrs={'class':'a-size-small a-link-normal'})\n",
        "        price = d.find('span', attrs={'class':'p13n-sc-price'})\n",
        "\n",
        "        all1=[]\n",
        "\n",
        "        if name is not None:\n",
        "            #print(n[0]['alt'])\n",
        "            all1.append(n[0]['alt'])\n",
        "        else:\n",
        "            all1.append(\"unknown-product\")\n",
        "\n",
        "        if author is not None:\n",
        "            #print(author.text)\n",
        "            all1.append(author.text)\n",
        "        elif author is None:\n",
        "            author = d.find('span', attrs={'class':'a-size-small a-color-base'})\n",
        "            if author is not None:\n",
        "                all1.append(author.text)\n",
        "            else:\n",
        "                all1.append('0')\n",
        "\n",
        "        if rating is not None:\n",
        "            #print(rating.text)\n",
        "            all1.append(rating.text)\n",
        "        else:\n",
        "            all1.append('-1')\n",
        "\n",
        "        if users_rated is not None:\n",
        "            #print(price.text)\n",
        "            all1.append(users_rated.text)\n",
        "        else:\n",
        "            all1.append('0')\n",
        "\n",
        "        if price is not None:\n",
        "            #print(price.text)\n",
        "            all1.append(price.text)\n",
        "        else:\n",
        "            all1.append('0')\n",
        "        alls.append(all1)\n",
        "    return alls"
      ],
      "metadata": {
        "id": "HWrMKq1AQIH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for i in range(1, no_pages+1):\n",
        "    results.append(get_data(i))\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "df = pd.DataFrame(flatten(results),columns=['Book Name','Author','Rating','Customers_Rated', 'Price'])\n",
        "df.to_csv('amazon_products.csv', index=False, encoding='utf-8')"
      ],
      "metadata": {
        "id": "MWuCS9ArQMUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrape the details most watched tv series of all time from imdb.com.\n",
        "Url = https://www.imdb.com/list/ls095964455/"
      ],
      "metadata": {
        "id": "QYSjOL7iQkLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Downloading imdb top 250 movie's data\n",
        "url = 'http://www.imdb.com/chart/top'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "movies = soup.select('td.titleColumn')\n",
        "crew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\n",
        "ratings = [b.attrs.get('data-value')\n",
        "\t\tfor b in soup.select('td.posterColumn span[name=ir]')]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# create a empty list for storing\n",
        "# movie information\n",
        "list = []\n",
        "\n",
        "# Iterating over movies to extract\n",
        "# each movie's details\n",
        "for index in range(0, len(movies)):\n",
        "\n",
        "\t# Separating movie into: 'place',\n",
        "\t# 'title', 'year'\n",
        "\tmovie_string = movies[index].get_text()\n",
        "\tmovie = (' '.join(movie_string.split()).replace('.', ''))\n",
        "\tmovie_title = movie[len(str(index))+1:-7]\n",
        "\tyear = re.search('\\((.*?)\\)', movie_string).group(1)\n",
        "\tplace = movie[:len(str(index))-(len(movie))]\n",
        "\tdata = {\"place\": place,\n",
        "\t\t\t\"movie_title\": movie_title,\n",
        "\t\t\t\"rating\": ratings[index],\n",
        "\t\t\t\"year\": year,\n",
        "\t\t\t\"star_cast\": crew[index],\n",
        "\t\t\t}\n",
        "\tlist.append(data)\n",
        "\n",
        "# printing movie details with its rating.\n",
        "for movie in list:\n",
        "\tprint(movie['place'], '-', movie['movie_title'], '('+movie['year'] +\n",
        "\t\t') -', 'Starring:', movie['star_cast'], movie['rating'])\n",
        "\n",
        "\n",
        "##.......##\n",
        "df = pd.DataFrame(list)\n",
        "df.to_csv('imdb_top_250_movies.csv',index=False)\n"
      ],
      "metadata": {
        "id": "xVsdcI0mQk3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Details of Datasets from UCI machine learning repositories.\n",
        "Url = https://archive.ics.uci.edu"
      ],
      "metadata": {
        "id": "GzjOowylQ6b1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pages = np.arange(1, 9951, 50) # Last time I tried, I could only go to 10000 items because after that the URI has no discernable pattern to combat webcrawlers; I just did 4 pages for demonstration purposes. You can increase this for your own projects.\n",
        "headers = {'Accept-Language': 'en-US,en;q=0.8'} # If this is not specified, the default language is Mandarin\n",
        "\n",
        "#initialize empty lists to store the variables scraped\n",
        "titles = []\n",
        "years = []\n",
        "ratings = []\n",
        "genres = []\n",
        "runtimes = []\n",
        "imdb_ratings = []\n",
        "imdb_ratings_standardized = []\n",
        "metascores = []\n",
        "votes = []\n",
        "\n",
        "for page in pages:\n",
        "\n",
        "   #get request for sci-fi\n",
        "   response = get(\"https://www.imdb.com/search/title?genres=sci-fi&\"\n",
        "                  + \"start=\"\n",
        "                  + str(page)\n",
        "                  + \"&explore=title_type,genres&ref_=adv_prv\", headers=headers)\n",
        "\n",
        "   sleep(randint(8,15))\n",
        "\n",
        "   #throw warning for status codes that are not 200\n",
        "   if response.status_code != 200:\n",
        "       warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
        "\n",
        "   #parse the content of current iteration of request\n",
        "   page_html = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "   movie_containers = page_html.find_all('div', class_ = 'lister-item mode-advanced')\n",
        "\n",
        "   #extract the 50 movies for that page\n",
        "   for container in movie_containers:\n",
        "\n",
        "       #conditional for all with metascore\n",
        "       if container.find('div', class_ = 'ratings-metascore') is not None:\n",
        "\n",
        "           #title\n",
        "           title = container.h3.a.text\n",
        "           titles.append(title)\n",
        "\n",
        "           if container.h3.find('span', class_= 'lister-item-year text-muted unbold') is not None:\n",
        "\n",
        "             #year released\n",
        "             year = container.h3.find('span', class_= 'lister-item-year text-muted unbold').text # remove the parentheses around the year and make it an integer\n",
        "             years.append(year)\n",
        "\n",
        "           else:\n",
        "             years.append(None) # each of the additional if clauses are to handle type None data, replacing it with an empty string so the arrays are of the same length at the end of the scraping\n",
        "\n",
        "           if container.p.find('span', class_ = 'certificate') is not None:\n",
        "\n",
        "             #rating\n",
        "             rating = container.p.find('span', class_= 'certificate').text\n",
        "             ratings.append(rating)\n",
        "\n",
        "           else:\n",
        "             ratings.append(\"\")\n",
        "\n",
        "           if container.p.find('span', class_ = 'genre') is not None:\n",
        "\n",
        "             #genre\n",
        "             genre = container.p.find('span', class_ = 'genre').text.replace(\"\\n\", \"\").rstrip().split(',') # remove the whitespace character, strip, and split to create an array of genres\n",
        "             genres.append(genre)\n",
        "\n",
        "           else:\n",
        "             genres.append(\"\")\n",
        "\n",
        "           if container.p.find('span', class_ = 'runtime') is not None:\n",
        "\n",
        "             #runtime\n",
        "             time = int(container.p.find('span', class_ = 'runtime').text.replace(\" min\", \"\")) # remove the minute word from the runtime and make it an integer\n",
        "             runtimes.append(time)\n",
        "\n",
        "           else:\n",
        "             runtimes.append(None)\n",
        "\n",
        "           if float(container.strong.text) is not None:\n",
        "\n",
        "             #IMDB ratings\n",
        "             imdb = float(container.strong.text) # non-standardized variable\n",
        "             imdb_ratings.append(imdb)\n",
        "\n",
        "           else:\n",
        "             imdb_ratings.append(None)\n",
        "\n",
        "           if container.find('span', class_ = 'metascore').text is not None:\n",
        "\n",
        "             #Metascore\n",
        "             m_score = int(container.find('span', class_ = 'metascore').text) # make it an integer\n",
        "             metascores.append(m_score)\n",
        "\n",
        "           else:\n",
        "             metascores.append(None)\n",
        "\n",
        "           if container.find('span', attrs = {'name':'nv'})['data-value'] is not None:\n",
        "\n",
        "             #Number of votes\n",
        "             vote = int(container.find('span', attrs = {'name':'nv'})['data-value'])\n",
        "             votes.append(vote)\n",
        "\n",
        "           else:\n",
        "               votes.append(None)\n",
        "\n",
        "           else:\n",
        "               votes.append(None)"
      ],
      "metadata": {
        "id": "1Md0o1RnQ7Wm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}