{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        ". In the above question, now scrape the following details of each product listed in first 3 pages of your search\n",
        "results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then\n",
        "scrape all the products available under that product name. Details to be scraped are: \"Brand\n",
        "Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and\n",
        "“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
      ],
      "metadata": {
        "id": "Z-ZG0O8vTEna"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aJkJMgcqTClG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selectorlib import Extractor\n",
        "import requests\n",
        "import json\n",
        "from time import sleep\n",
        "\n",
        "\n",
        "# Create an Extractor by reading from the YAML file\n",
        "e = Extractor.from_yaml_file('selectors.yml')\n",
        "\n",
        "def scrape(url):\n",
        "\n",
        "    headers = {\n",
        "        'dnt': '1',\n",
        "        'upgrade-insecure-requests': '1',\n",
        "        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36',\n",
        "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
        "        'sec-fetch-site': 'same-origin',\n",
        "        'sec-fetch-mode': 'navigate',\n",
        "        'sec-fetch-user': '?1',\n",
        "        'sec-fetch-dest': 'document',\n",
        "        'referer': 'https://www.amazon.com/',\n",
        "        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
        "    }\n",
        "\n",
        "    # Download the page using requests\n",
        "    print(\"Downloading %s\"%url)\n",
        "    r = requests.get(url, headers=headers)\n",
        "    # Simple check to check if page was blocked (Usually 503)\n",
        "    if r.status_code > 500:\n",
        "        if \"To discuss automated access to Amazon data please contact\" in r.text:\n",
        "            print(\"Page %s was blocked by Amazon. Please try using better proxies\\n\"%url)\n",
        "        else:\n",
        "            print(\"Page %s must have been blocked by Amazon as the status code was %d\"%(url,r.status_code))\n",
        "        return None\n",
        "    # Pass the HTML of the page and create\n",
        "    return e.extract(r.text)\n",
        "\n",
        "# product_data = []\n",
        "with open(\"urls.txt\",'r') as urllist, open('output.jsonl','w') as outfile:\n",
        "    for url in urllist.read().splitlines():\n",
        "        data = scrape(url)\n",
        "        if data:\n",
        "            json.dump(data,outfile)\n",
        "            outfile.write(\"\\n\")\n",
        "            # sleep(5)"
      ],
      "metadata": {
        "id": "MtJjS97VJQ8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "{\"name\": \"2020 HP 15.6\\\" Laptop Computer, 10th Gen Intel Quard-Core i7 1065G7 up to 3.9GHz, 16GB DDR4 RAM, 512GB PCIe SSD, 802.11ac WiFi, Bluetooth 4.2, Silver, Windows 10, YZAKKA USB External DVD + Accessories\", \"price\": \"Price: $959.00\", \"short_description\": null, \"images\": \"{\\\"https://images-na.ssl-images-amazon.com/images/I/61CBqERgZ7L._AC_SX425_.jpg\\\":[425,425],\\\"https://images-na.ssl-images-amazon.com/images/I/61CBqERgZ7L._AC_SX466_.jpg\\\":[466,466],\\\"https://images-na.ssl-images-amazon.com/images/I/61CBqERgZ7L._AC_SY355_.jpg\\\":[355,355],\\\"https://images-na.ssl-images-amazon.com/images/I/61CBqERgZ7L._AC_SX569_.jpg\\\":[569,569],\\\"https://images-na.ssl-images-amazon.com/images/I/61CBqERgZ7L._AC_SY450_.jpg\\\":[450,450],\\\"https://images-na.ssl-images-amazon.com/images/I/61CBqERgZ7L._AC_SX679_.jpg\\\":[679,679],\\\"https://images-na.ssl-images-amazon.com/images/I/61CBqERgZ7L._AC_SX522_.jpg\\\":[522,522]}\", \"rating\": null, \"number_of_reviews\": null, \"variants\": [{\"name\": \"Click to select 4GB DDR4 RAM, 128GB PCIe SSD\", \"asin\": \"B01MCZ4LH1\"}, {\"name\": \"Click to select 8GB DDR4 RAM, 256GB PCIe SSD\", \"asin\": \"B08537NR9D\"}, {\"name\": \"Click to select 12GB DDR4 RAM, 512GB PCIe SSD\", \"asin\": \"B08537ZDYH\"}, {\"name\": \"Click to select 16GB DDR4 RAM, 512GB PCIe SSD\", \"asin\": \"B085383P7M\"}, {\"name\": \"Click to select 20GB DDR4 RAM, 1TB PCIe SSD\", \"asin\": \"B08537NDVZ\"}], \"sizes\": null, \"product_description\": \"Capacity: 16GB DDR4 RAM, 512GB PCIe SSD Processor Intel Core i7-1065G7 (1.3 GHz base frequency, up to 3.9 GHz with Intel Turbo Boost Technology, 8 MB cache, 4 cores) Chipset Intel Integrated SoC Memory 16GB DDR4-2666 SDRAM Video graphics Intel Iris Plus Graphics Hard drive 512GB PCIe NVMe M.2 SSD Display 15.6\\\" diagonal HD SVA BrightView micro-edge WLED-backlit, 220 nits, 45% NTSC (1366 x 768) Wireless connectivity Realtek RTL8821CE 802.11b/g/n/ac (1x1) Wi-Fi and Bluetooth 4.2 Combo Expansion slots 1 multi-format SD media card reader External ports 1 USB 3.1 Gen 1 Type-C (Data Transfer Only, 5 Gb/s signaling rate); 2 USB 3.1 Gen 1 Type-A (Data Transfer Only); 1 AC smart pin; 1 HDMI 1.4b; 1 headphone/microphone combo Minimum dimensions (W x D x H) 9.53 x 14.11 x 0.70 in Weight 3.75 lbs Power supply type 45 W Smart AC power adapter Battery type 3-cell, 41 Wh Li-ion Battery life mixed usage Up to 11 hours and 30 minutes Video Playback Battery life Up to 10 hours Webcam HP TrueVision HD Camera with integrated dual array digital microphone Audio features Dual speakers Operating system Windows 10 Home 64 Accessories YZAKKA USB External DVD drive + USB extension cord 6ft, HDMI cable 6ft and Mouse Pad\", \"product_attributes\": null, \"sales_rank\": null, \"link_to_all_reviews\": \"/HP-Computer-Quard-Core-Bluetooth-Accessories/product-reviews/B085383P7M/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\", \"from_manufacturer\": \"\"}\n",
        "{\"name\": \"2020 HP 15.6\\\" Touchscreen Laptop Computer, Quad-Core AMD Ryzen 7 3700U up to 4.0GHz, 12GB DDR4 RAM, 256GB PCIe SSD, 802.11ac WiFi, Bluetooth 4.2, USB 3.1 Type-C, HDMI, Silver, Windows 10 Home\", \"price\": null, \"short_description\": null, \"images\": \"{\\\"https://images-na.ssl-images-amazon.com/images/I/61caV0pK85L._AC_SX466_.jpg\\\":[324,466],\\\"https://images-na.ssl-images-amazon.com/images/I/61caV0pK85L._AC_SX355_.jpg\\\":[247,355],\\\"https://images-na.ssl-images-amazon.com/images/I/61caV0pK85L._AC_SX679_.jpg\\\":[472,679],\\\"https://images-na.ssl-images-amazon.com/images/I/61caV0pK85L._AC_SX522_.jpg\\\":[363,522],\\\"https://images-na.ssl-images-amazon.com/images/I/61caV0pK85L._AC_SX450_.jpg\\\":[313,450],\\\"https://images-na.ssl-images-amazon.com/images/I/61caV0pK85L._AC_SX425_.jpg\\\":[295,425],\\\"https://images-na.ssl-images-amazon.com/images/I/61caV0pK85L._AC_SX569_.jpg\\\":[395,569]}\", \"rating\": null, \"number_of_reviews\": null, \"variants\": null, \"sizes\": null, \"product_description\": \"Screen size 15.6 in HD SVA touchscreen (1366 x 768), 10-finger multi-touch support Processor AMD Ryzen 7 3700U 2.3 GHz up to 4.0 GHz Memory 12GB DDR4 2400 MHz Hard drive size 256GB PCIe NVMe SSD Operating system Windows 10 Home, 64-bit Optical drive None Media drive Multi-format SD media card reader Audio HD Audio with stereo speakers Video AMD Radeon RX Vega 10 Graphics Ports 1 USB 3.1 Gen 1 Type-C 2 USB 3.1 Gen 1 Type-A 1 HDMI 1.4 1 AC Smart pin 1 headphone/microphone combo jack Battery 41Whr 3-cell lithium-ion battery (up to 10 hours) Camera HP TrueVision HD camera Wireless Realtek RTL8821CE 802.11b/g/n/ac Bluetooth Bluetooth 4.2 Dimensions 14.11 x 9.53 x 0.78 in (104.39 x 242.06 x 19.81 mm) Weight 3.75 lbs (1.70 kgs) Color Natural silver\", \"product_attributes\": null, \"sales_rank\": null, \"link_to_all_reviews\": \"/HP-Touchscreen-Computer-Quad-Core-Bluetooth/product-reviews/B082HZBDXH/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\", \"from_manufacturer\": null}\n",
        "{\"name\": \"2020 Premium HP 15 Laptop Computer PC, 15.6 inch HD Touchscreen, 8th Gen Intel Dual-Core i3 8145U (>i5-7200U), 4GB DDR4 128GB SSD, WiFi BT 4.2 USB-C HDMI Win 10 S (Silver) + Delca 16GB Micro SD Card\", \"price\": \"Price: $479.00\", \"short_description\": null, \"images\": \"{\\\"https://images-na.ssl-images-amazon.com/images/I/61uDrxNBv0L._AC_SX679_.jpg\\\":[679,679],\\\"https://images-na.ssl-images-amazon.com/images/I/61uDrxNBv0L._AC_SY355_.jpg\\\":[355,355],\\\"https://images-na.ssl-images-amazon.com/images/I/61uDrxNBv0L._AC_SX522_.jpg\\\":[522,522],\\\"https://images-na.ssl-images-amazon.com/images/I/61uDrxNBv0L._AC_SY450_.jpg\\\":[450,450],\\\"https://images-na.ssl-images-amazon.com/images/I/61uDrxNBv0L._AC_SX466_.jpg\\\":[466,466],\\\"https://images-na.ssl-images-amazon.com/images/I/61uDrxNBv0L._AC_SX425_.jpg\\\":[425,425],\\\"https://images-na.ssl-images-amazon.com/images/I/61uDrxNBv0L._AC_SX569_.jpg\\\":[569,569]}\", \"rating\": null, \"number_of_reviews\": null, \"variants\": [{\"name\": \"Click to select 4GB DDR4 I 128GB SSD\", \"asin\": \"B0863N5FM8\"}, {\"name\": \"Click to select 4GB DDR4 I 512GB SSD\", \"asin\": \"B0863N6KD9\"}, {\"name\": \"Click to select 8GB DDR4 I 1TB SSD\", \"asin\": \"B0863GRMGS\"}, {\"name\": \"Click to select 8GB DDR4 I 128GB SSD\", \"asin\": \"B0863H65FZ\"}, {\"name\": \"Click to select 8GB DDR4 I 256GB SSD\", \"asin\": \"B0863F939M\"}, {\"name\": \"Click to select 8GB DDR4 I 512GB SSD\", \"asin\": \"B086365T17\"}, {\"name\": \"Click to select 16GB DDR4 I 1TB SSD\", \"asin\": \"B08635SNBP\"}, {\"name\": \"Click to select 16GB DDR4 I 256GB SSD\", \"asin\": \"B0863KC87T\"}, {\"name\": \"Click to select 16GB DDR4 I 512GB SSD\", \"asin\": \"B0863CSQPQ\"}, {\"name\": \"Click to select Gold\", \"asin\": \"B0863C21NF\"}, {\"name\": \"Click to select Silver\", \"asin\": \"B0863N5FM8\"}], \"sizes\": null, \"product_description\": \"Capacity: 4GB DDR4 I 128GB SSD |\\u00a0\\n            \\n             Color: Silver This listing by Apricot\\u00a0Power\\u00a0PC\\u00a0sells\\u00a0computers\\u00a0with\\u00a0upgraded configurations.If the computer has modifications (listed above), then the manufacturer box is opened for it to be tested and inspected and to install the upgrades to achieve the specifications as advertised. If no modification are listed, the item is unopened and untested. Defects & blemishes are significantly reduced by our in depth inspection & testing PRODUCT OVERVIEW: Stay connected to what matters most with long-lasting battery life and a sleek and portable, micro-edge bezel design with the HP 15.6\\u201d Touchscreen Laptop. Built to keep you productive and entertained from anywhere, the HP 15-inch laptop features reliable performance and an expansive display - letting you stream, surf and speed through tasks from sun up to sun down. KEY SPECIFICATIONS: PC Type: Traditional Notebook Laptop Computer PC Series: HP 15.6 i3 Display: 15.6-inch HD (1366 x 768) Touchscreen Micro-Edge Display Processor: Intel Dual-Core i3-8145U (>i5-7200U), 2.1 GHz up to 3.9GHz, 4 MB Intel Smart Cache, 4 Threads Memory: 4GB DDR4 SSD: 128GB Solid State Drive Graphics: Integrated Intel UHD Graphics 620 Communications: 802.11ac and Bluetooth 4.2 Camera: Built-in HD Webcam Operating system: Windows 10 Ports & Slots: 1 x USB 3.1 Gen 1 Type-C , 2 x USB 3.1 Gen 1, 1 x AC smart pin, 1 x HDMI, 1 x RJ-45, 1 x headphone/microphone combo, 1 x multi-format SD media card reader Additional Information: Dimensions: 14.11\\\" x 9.53\\\" x 0.78\\\" Approximate Weight: 3.84lbs Color: Silver Accessory: DELCA\\u00a016GB\\u00a0Microso\\u00a0SD\\u00a0included\", \"product_attributes\": null, \"sales_rank\": null, \"link_to_all_reviews\": null, \"from_manufacturer\": \"\"}"
      ],
      "metadata": {
        "id": "rqHeF4_WJWj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "products:\n",
        "    css: 'div[data-component-type=\"s-search-result\"]'\n",
        "    xpath: null\n",
        "    multiple: true\n",
        "    type: Text\n",
        "    children:\n",
        "        title:\n",
        "            css: 'h2 a.a-link-normal.a-text-normal'\n",
        "            xpath: null\n",
        "            type: Text\n",
        "        url:\n",
        "            css: 'h2 a.a-link-normal.a-text-normal'\n",
        "            xpath: null\n",
        "            type: Link\n",
        "        rating:\n",
        "            css: 'div.a-row.a-size-small span:nth-of-type(1)'\n",
        "            xpath: null\n",
        "            type: Attribute\n",
        "            attribute: aria-label\n",
        "        reviews:\n",
        "            css: 'div.a-row.a-size-small span:nth-of-type(2)'\n",
        "            xpath: null\n",
        "            type: Attribute\n",
        "            attribute: aria-label\n",
        "        price:\n",
        "            css: 'span.a-price:nth-of-type(1) span.a-offscreen'\n",
        "            xpath: null\n",
        "            type: Text"
      ],
      "metadata": {
        "id": "FNAzsjuXJecy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com\n",
        "and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand\n",
        "Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,\n",
        "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the\n",
        "details is missing then replace it by “- “. Save your results in a dataframe and CSV."
      ],
      "metadata": {
        "id": "838ohnvVSxbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selectorlib import Extractor\n",
        "import requests\n",
        "import json\n",
        "from time import sleep\n",
        "\n",
        "\n",
        "# Create an Extractor by reading from the YAML file\n",
        "e = Extractor.from_yaml_file('search_results.yml')\n",
        "\n",
        "def scrape(url):\n",
        "\n",
        "    headers = {\n",
        "        'dnt': '1',\n",
        "        'upgrade-insecure-requests': '1',\n",
        "        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36',\n",
        "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
        "        'sec-fetch-site': 'same-origin',\n",
        "        'sec-fetch-mode': 'navigate',\n",
        "        'sec-fetch-user': '?1',\n",
        "        'sec-fetch-dest': 'document',\n",
        "        'referer': 'https://www.amazon.com/',\n",
        "        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
        "    }\n",
        "\n",
        "    # Download the page using requests\n",
        "    print(\"Downloading %s\"%url)\n",
        "    r = requests.get(url, headers=headers)\n",
        "    # Simple check to check if page was blocked (Usually 503)\n",
        "    if r.status_code > 500:\n",
        "        if \"To discuss automated access to Amazon data please contact\" in r.text:\n",
        "            print(\"Page %s was blocked by Amazon. Please try using better proxies\\n\"%url)\n",
        "        else:\n",
        "            print(\"Page %s must have been blocked by Amazon as the status code was %d\"%(url,r.status_code))\n",
        "        return None\n",
        "    # Pass the HTML of the page and create\n",
        "    return e.extract(r.text)\n",
        "\n",
        "# product_data = []\n",
        "with open(\"search_results_urls.txt\",'r') as urllist, open('search_results_output.jsonl','w') as outfile:\n",
        "    for url in urllist.read().splitlines():\n",
        "        data = scrape(url)\n",
        "        if data:\n",
        "            for product in data['products']:\n",
        "                product['search_url'] = url\n",
        "                print(\"Saving Product: %s\"%product['title'])\n",
        "                json.dump(product,outfile)\n",
        "                outfile.write(\"\\n\")\n",
        "                # sleep(5)\n",
        ""
      ],
      "metadata": {
        "id": "7psxoxNdJkiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name:\n",
        "    css: '#productTitle'\n",
        "    type: Text\n",
        "price:\n",
        "    css: '#price_inside_buybox'\n",
        "    type: Text\n",
        "short_description:\n",
        "    css: '#featurebullets_feature_div'\n",
        "    type: Text\n",
        "images:\n",
        "    css: '.imgTagWrapper img'\n",
        "    type: Attribute\n",
        "    attribute: data-a-dynamic-image\n",
        "rating:\n",
        "    css: span.arp-rating-out-of-text\n",
        "    type: Text\n",
        "number_of_reviews:\n",
        "    css: 'a.a-link-normal h2'\n",
        "    type: Text\n",
        "variants:\n",
        "    css: 'form.a-section li'\n",
        "    multiple: true\n",
        "    type: Text\n",
        "    children:\n",
        "        name:\n",
        "            css: \"\"\n",
        "            type: Attribute\n",
        "            attribute: title\n",
        "        asin:\n",
        "            css: \"\"\n",
        "            type: Attribute\n",
        "            attribute: data-defaultasin\n",
        "product_description:\n",
        "    css: '#productDescription'\n",
        "    type: Text\n",
        "sales_rank:\n",
        "    css: 'li#SalesRank'\n",
        "    type: Text\n",
        "link_to_all_reviews:\n",
        "    css: 'div.card-padding a.a-link-emphasis'\n",
        "    type: Link"
      ],
      "metadata": {
        "id": "nH1_YabKKGOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pd.set_option('display.max_columns', 60)\n",
        "\n",
        "acnc = pd.read_excel('data/datadotgov_main.xlsx', keep_default_na=False)"
      ],
      "metadata": {
        "id": "qyac1i2hKso1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acnc.head()"
      ],
      "metadata": {
        "id": "2WFdzuYqKvzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mel = acnc[acnc.Town_City.str.contains('melbourne', case=False)][['ABN', 'Charity_Legal_Name', 'Address_Line_1',\n",
        "                                                                  'Address_Line_2', 'Address_Line_3', 'Town_City',\n",
        "                                                                  'State', 'Postcode', 'Country',\n",
        "                                                                  'Date_Organisation_Established', 'Charity_Size']].copy()"
      ],
      "metadata": {
        "id": "mEum37RUKy7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mel.Town_City.value_counts()"
      ],
      "metadata": {
        "id": "THXB43tiK26F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mel.head()"
      ],
      "metadata": {
        "id": "iB-u9qYJMLpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mel['Full_Address'] = mel['Address_Line_1'].str.cat( mel[['Address_Line_2', 'Address_Line_3', 'Town_City']], sep=' ')"
      ],
      "metadata": {
        "id": "EsTsL-RvMMGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mel.Full_Address.iloc[0]"
      ],
      "metadata": {
        "id": "UFPH5q5bMQuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mel[mel.Full_Address.str.contains('po box', case=False)].Full_Address.iloc[0]"
      ],
      "metadata": {
        "id": "GIe-dGyqMTFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mel = mel[~mel.Full_Address.str.contains('po box', case=False)].copy()"
      ],
      "metadata": {
        "id": "KDUi-GKxMfBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mel.Full_Address = mel.Full_Address.str.replace('/', ' ')"
      ],
      "metadata": {
        "id": "AdnLvDN5MjOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F9xtm70FSlXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a program to scrap all the available details of best gaming laptops from digit.in."
      ],
      "metadata": {
        "id": "kofrQJwJSp_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import selenium\n",
        "from selenium import webdriver\n",
        "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
        "import time\n",
        "import re"
      ],
      "metadata": {
        "id": "rJRST2w3MlfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "driver = webdriver.Chrome(r\"E:\\Aniket\\chromedriver_win32\\chromedriver.exe\")"
      ],
      "metadata": {
        "id": "Iyj8V8BUNPWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url=\"https://www.digit.in/top-products/best-gaming-laptops-40.html\""
      ],
      "metadata": {
        "id": "naP5xpZZNS2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "driver.get(url)"
      ],
      "metadata": {
        "id": "19HgZGPJNVhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Brands=[]\n",
        "Products_Description=[]\n",
        "Specification=[]\n",
        "Price=[]"
      ],
      "metadata": {
        "id": "xICooNRSN8y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "br=driver.find_elements_by_xpath(\"//div[@class='TopNumbeHeading active sticky-footer']\")\n",
        "len(br)"
      ],
      "metadata": {
        "id": "MOl5SiAAN_At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in br:\n",
        "\n",
        "    Brands.append(str(i.text).replace(\"\\n\",\"\"))\n",
        "Brands"
      ],
      "metadata": {
        "id": "BMT89xo3OCUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp=driver.find_elements_by_xpath(\"//div[@class='Specs-Wrap']\")\n",
        "len(sp)"
      ],
      "metadata": {
        "id": "VvuJHrWMOE8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in sp:\n",
        "\n",
        "    Specification.append(str(i.text).replace(\"\\n\",\"\"))\n",
        "Specification"
      ],
      "metadata": {
        "id": "lY6_0FQoOKP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "des=driver.find_elements_by_xpath(\"//div[@class='Section-center']\")\n",
        "len(des)"
      ],
      "metadata": {
        "id": "Gk3QYZEaOMXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in des:\n",
        "\n",
        "    Products_Description.append(str(i.text).replace(\"\\n\",\"\"))\n",
        "Products_Description"
      ],
      "metadata": {
        "id": "fD7lkbg_OONR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pri=driver.find_elements_by_xpath(\"//td[@class='smprice']\")\n",
        "len(pri)"
      ],
      "metadata": {
        "id": "FuCzyl_POTnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in pri:\n",
        "\n",
        "    Price.append(str(i.text).replace(\"\\n\",\"\"))\n",
        "Price"
      ],
      "metadata": {
        "id": "cw2gtmjhOVyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "digit_lap=pd.DataFrame([])\n",
        "digit_lap['Brands']=Brands[0:10]\n",
        "digit_lap['Price']=Price[0:10]\n",
        "digit_lap['Specification']=Specification[0:10]\n",
        "digit_lap['Description']=Products_Description[0:10]\n",
        "digit_lap"
      ],
      "metadata": {
        "id": "_us7jbdkO-7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped:\n",
        "“Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”.\n"
      ],
      "metadata": {
        "id": "GZ-JwvE6SeaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas.util"
      ],
      "metadata": {
        "id": "qNPjD2w5P_X7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forbes = pd.read_csv('reviews_static.csv', names = ['Rank', 'Name', 'Net Worth', 'Age', 'Source', 'Country'])\n",
        "\n",
        "forbes_realtime = pd.read_csv('reviews_realtime.csv', names = ['Rank', 'Name', 'Net Worth', 'Age', 'Source', 'Country'])\n",
        "\n",
        "forbes_women = pd.read_csv('reviews_static_women.csv', names = ['Rank', 'Name', 'Net Worth', 'Age', 'Source', 'Country'])"
      ],
      "metadata": {
        "id": "8-DATfV8P_4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating data frame for current net worth\n",
        "realtime = forbes_realtime[['Name','Net Worth']]\n",
        "\n",
        "#merging dataframe\n",
        "forbes_data = pd.merge(forbes, realtime,how = 'left', on = 'Name')\n",
        "\n",
        "#cleaning NA values\n",
        "forbes_data['Age'] = forbes_data['Age'].fillna(forbes_data['Age'].median())\n",
        "forbes_data['Net Worth_y'] = forbes_data['Net Worth_y'].fillna(0)\n",
        "\n",
        "#creating new column \"Change\"\n",
        "forbes_data['Change'] = forbes_data['Net Worth_y']- forbes_data['Net Worth_x']"
      ],
      "metadata": {
        "id": "ol8jtNXFQDU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# changing float into integar\n",
        "forbes_data['Age'] = list(map(lambda x: int(x), forbes_data['Age']))\n",
        "forbes_data['Rank'] = list(map(lambda x: int(x), forbes_data['Rank']))"
      ],
      "metadata": {
        "id": "muaDWFZ6QHNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating women data frame, so i can merge that into main data frame to create Gender column\n",
        "women = forbes_women['Name']\n",
        "\n",
        "#Creating new column \"Gender\" in main data frame\n",
        "forbes_data['Gender'] = forbes_data['Name'].isin(women)\n",
        "forbes_data['Gender'].replace([True,False],['F','M'],inplace = True)"
      ],
      "metadata": {
        "id": "f5oi0yh1QL2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking final data frame\n",
        "forbes_data.head(10)"
      ],
      "metadata": {
        "id": "xVlugv4JQMUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking Null values (No NA values)\n",
        "np.sum(forbes_data.isnull())"
      ],
      "metadata": {
        "id": "Ts3gZM3yQfDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "GXpLlsObQfjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import matplotlib for plotting graph\n",
        "from matplotlib import pyplot as plt\n",
        "plt.style.use('ggplot')"
      ],
      "metadata": {
        "id": "rwJsIa7KQjv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GRAPH 1\n",
        "#group by country\n",
        "B_by_country = forbes_data.groupby('Country')[['Name']].count()\n",
        "\n",
        "#plot bar Graph\n",
        "B_by_country.sort_values(by= 'Name', ascending = False).head(25).plot(kind= 'bar', figsize = (18,9),color = 'bbbbbcccccyyyyyrrrrr')\n",
        "plt.xlabel('Country (Top 25)',fontsize = 20)\n",
        "plt.ylabel('Number of Billionaires',fontsize = 20)\n",
        "plt.title('Number of billionaires by country', fontsize = 22)\n",
        "# plt.savefig(\"figure1.png\")"
      ],
      "metadata": {
        "id": "n4wWYh1VQkKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GRAPH 2.1\n",
        "#selecting Gender column and plotting graph\n",
        "forbes_data['Gender'].value_counts().plot.bar(figsize=(18, 9), colormap = 'viridis' )\n",
        "plt.xlabel('M = Male , F = Female',fontsize = 20)\n",
        "plt.ylabel('Number',fontsize = 20)\n",
        "plt.title('Number of Male & Female', fontsize = 22)\n",
        "# plt.savefig(\"figure2.png\")"
      ],
      "metadata": {
        "id": "9mz49u0cQm29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GRAPH 2.2\n",
        "#selecting Gender column and plotting graph\n",
        "forbes_data['Gender'].value_counts().plot.pie(figsize = (18,9),autopct='%1.1f%%',shadow=True,\\\n",
        "                                              explode = (0,0.1),labels =('Male', 'Female'),colors = 'br',).axis('equal')\n",
        "plt.xlabel('Male: 89.2 | Female:10.8', fontsize = 20)\n",
        "plt.title('Pie Chart: Male to Female Ratio', fontsize=22)\n",
        "plt.ylabel('Gender', fontsize = 20)\n",
        "# plt.savefig(\"figure3.png\")"
      ],
      "metadata": {
        "id": "ibsIL7owQqxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GRAPH 3\n",
        "#selecting Age column and plotting graph\n",
        "popular_source = forbes_data.groupby('Source')['Source'].count()\n",
        "popular_source.sort_values(ascending = False).head(20).plot.bar(figsize = (18,9), width = .9)\n",
        "plt.xlabel('Source of Income',fontsize=20)\n",
        "plt.ylabel('Frequency',fontsize=20)\n",
        "plt.title('Bar plot for popular source of Income ', fontsize=20)\n",
        "# plt.savefig(\"figure3.png\")\n",
        ""
      ],
      "metadata": {
        "id": "xsa7L71oQyAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GRAPH 4\n",
        "#selecting Age column and plotting graph\n",
        "forbes_data['Age'].plot(kind = 'hist', bins =25, color =\"#5ee3ff\",stacked = True, figsize = (18,9) )\n",
        "plt.xlabel('Age (Min. = 22 | Max. = 100 | mean = 64)',fontsize=20)\n",
        "plt.ylabel('Frequency',fontsize=20)\n",
        "plt.title('Histogram of Age', fontsize=20)\n",
        "# plt.savefig(\"figure4.png\")"
      ],
      "metadata": {
        "id": "SqEx3Ml2R7sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GRAPH 5\n",
        "\n",
        "#selecting Country and Net Worth_y column\n",
        "Temp= forbes_data.groupby('Country')[['Net Worth_y']].agg('max').reset_index()\n",
        "\n",
        "#making a data frame by one richest person from each country\n",
        "B_per_country = pd.merge(forbes_data, Temp, how = 'inner', on = [\"Country\", \"Net Worth_y\"])\n",
        "\n",
        "#plotting graph\n",
        "B_per_country.head(11).plot(kind ='bar',x ='Country', y = 'Net Worth_y', figsize = (18,9), width = .9)\n",
        "plt.xlabel('COUNTRY NAME (TOP 11)',fontsize=20)\n",
        "plt.ylabel('REAL TIME NET WORTH (IN BILLIONS)',fontsize=20)\n",
        "plt.title('RICHEST PERSON FROM EACH COUNTRY', fontsize=20)\n",
        "# plt.savefig(\"figure5.png\")"
      ],
      "metadata": {
        "id": "UAQOyIaKR-_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GRAPH 6\n",
        "#plotting graph for change in net_worth and real time net_worth\n",
        "forbes_data.head(11).plot(figsize = (18,9),x = 'Name',y=[\"Net Worth_x\", \"Net Worth_y\", \"Change\"], kind=\"bar\", width= .85, colors = 'bmy')\n",
        "plt.xlabel('NAME OF PERSON (TOP 11) Net Worth_x = 2018 Net Worth  |  Net Worth_y = Real Time Net Worth',fontsize=20)\n",
        "plt.ylabel('REAL TIME NET WORTH (IN BILLIONS)',fontsize=20)\n",
        "plt.title('CHANGE IN NET WORTH', fontsize=22)\n",
        "# plt.savefig(\"figure6.png\")"
      ],
      "metadata": {
        "id": "aSHXUwraSB33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GRAPH 7\n",
        "# plotting graph for total net worth for each country\n",
        "total_money = forbes_data.groupby('Country')[['Net Worth_x']].agg('sum').reset_index()\n",
        "\n",
        "#taking log of total net_worth\n",
        "total_money['Net Worth_x'] = np.log2(total_money['Net Worth_x'])\n",
        "\n",
        "#plotting graph\n",
        "total_money.head(10).sort_values(by = 'Net Worth_x', ascending = False).plot.bar(x = 'Country', y = 'Net Worth_x',figsize = (18,9),xlim = 20)\n",
        "plt.xlabel('Group by Country (Top 10)', fontsize = 20)\n",
        "plt.ylabel('log value of total net worth', fontsize = 20)\n",
        "plt.title('Bar Plot: Total money by country', fontsize=22)\n",
        "# plt.savefig(\"figure7.png\")"
      ],
      "metadata": {
        "id": "OcO5NTV8SEwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gs-wEDD2SIN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted\n",
        "from any YouTube Video."
      ],
      "metadata": {
        "id": "KBJhXbX8SViU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Code is partially grabbed from this repository:\n",
        "# https://github.com/egbertbouman/youtube-comment-downloader\n",
        "\n",
        "def search_dict(partial, key):\n",
        "    \"\"\"\n",
        "    A handy function that searches for a specific `key` in a `data` dictionary/list\n",
        "    \"\"\"\n",
        "    if isinstance(partial, dict):\n",
        "        for k, v in partial.items():\n",
        "            if k == key:\n",
        "                # found the key, return the value\n",
        "                yield v\n",
        "            else:\n",
        "                # value of the dict may be another dict, so we search there again\n",
        "                for o in search_dict(v, key):\n",
        "                    yield o\n",
        "    elif isinstance(partial, list):\n",
        "        # if the passed data is a list\n",
        "        # iterate over it & search for the key at the items in the list\n",
        "        for i in partial:\n",
        "            for o in search_dict(i, key):\n",
        "                yield o\n",
        "\n",
        "\n",
        "def find_value(html, key, num_sep_chars=2, separator='\"'):\n",
        "    # define the start position by the position of the key +\n",
        "    # length of key + separator length (usually : and \")\n",
        "    start_pos = html.find(key) + len(key) + num_sep_chars\n",
        "    # the end position is the position of the separator (such as \")\n",
        "    # starting from the start_pos\n",
        "    end_pos = html.find(separator, start_pos)\n",
        "    # return the content in this range\n",
        "    return html[start_pos:end_pos]\n",
        "\n",
        "\n",
        "def get_comments(url):\n",
        "    session = requests.Session()\n",
        "    # make the request\n",
        "    res = session.get(url)\n",
        "    # extract the XSRF token\n",
        "    xsrf_token = find_value(res.text, \"XSRF_TOKEN\", num_sep_chars=3)\n",
        "    # parse the YouTube initial data in the <script> tag\n",
        "    data_str = find_value(res.text, 'window[\"ytInitialData\"] = ', num_sep_chars=0, separator=\"\\n\").rstrip(\";\")\n",
        "    # convert to Python dictionary instead of plain text string\n",
        "    data = json.loads(data_str)\n",
        "    # search for the ctoken & continuation parameter fields\n",
        "    for r in search_dict(data, \"itemSectionRenderer\"):\n",
        "        pagination_data = next(search_dict(r, \"nextContinuationData\"))\n",
        "        if pagination_data:\n",
        "            # if we got something, break out of the loop,\n",
        "            # we have the data we need\n",
        "            break\n",
        "\n",
        "    continuation_tokens = [(pagination_data['continuation'], pagination_data['clickTrackingParams'])]\n",
        "\n",
        "    while continuation_tokens:\n",
        "        # keep looping until continuation tokens list is empty (no more comments)\n",
        "        continuation, itct = continuation_tokens.pop()\n",
        "\n",
        "        # construct params parameter (the ones in the URL)\n",
        "        params = {\n",
        "            \"action_get_comments\": 1,\n",
        "            \"pbj\": 1,\n",
        "            \"ctoken\": continuation,\n",
        "            \"continuation\": continuation,\n",
        "            \"itct\": itct,\n",
        "        }\n",
        "\n",
        "        # construct POST body data, which consists of the XSRF token\n",
        "        data = {\n",
        "            \"session_token\": xsrf_token,\n",
        "        }\n",
        "\n",
        "        # construct request headers\n",
        "        headers = {\n",
        "            \"x-youtube-client-name\": \"1\",\n",
        "            \"x-youtube-client-version\": \"2.20200731.02.01\"\n",
        "        }\n",
        "\n",
        "        # make the POST request to get the comments data\n",
        "        response = session.post(\"https://www.youtube.com/comment_service_ajax\", params=params, data=data, headers=headers)\n",
        "        # convert to a Python dictionary\n",
        "        comments_data = json.loads(response.text)\n",
        "\n",
        "        for comment in search_dict(comments_data, \"commentRenderer\"):\n",
        "            # iterate over loaded comments and yield useful info\n",
        "            yield {\n",
        "                \"commentId\": comment[\"commentId\"],\n",
        "                \"text\": ''.join([c['text'] for c in comment['contentText']['runs']]),\n",
        "                \"time\": comment['publishedTimeText']['runs'][0]['text'],\n",
        "                \"isLiked\": comment[\"isLiked\"],\n",
        "                \"likeCount\": comment[\"likeCount\"],\n",
        "                # \"replyCount\": comment[\"replyCount\"],\n",
        "                'author': comment.get('authorText', {}).get('simpleText', ''),\n",
        "                'channel': comment['authorEndpoint']['browseEndpoint']['browseId'],\n",
        "                'votes': comment.get('voteCount', {}).get('simpleText', '0'),\n",
        "                'photo': comment['authorThumbnail']['thumbnails'][-1]['url'],\n",
        "                \"authorIsChannelOwner\": comment[\"authorIsChannelOwner\"],\n",
        "            }\n",
        "\n",
        "        # load continuation tokens for next comments (ctoken & itct)\n",
        "        continuation_tokens = [(next_cdata['continuation'], next_cdata['clickTrackingParams'])\n",
        "                         for next_cdata in search_dict(comments_data, 'nextContinuationData')] + continuation_tokens\n",
        "\n",
        "        # avoid heavy loads with popular videos\n",
        "        time.sleep(0.1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # from pprint import pprint\n",
        "    # url = \"https://www.youtube.com/watch?v=jNQXAC9IVRw\"\n",
        "    # for count, comment in enumerate(get_comments(url)):\n",
        "    #     if count == 3:\n",
        "    #         break\n",
        "    #     pprint(comment)\n",
        "    #     print(\"=\"*50)\n",
        "    import argparse\n",
        "    import os\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Simple YouTube Comment extractor\")\n",
        "    parser.add_argument(\"url\", help=\"The YouTube video full URL\")\n",
        "    parser.add_argument(\"-l\", \"--limit\", type=int, help=\"Number of maximum comments to extract, helpful for longer videos\")\n",
        "    parser.add_argument(\"-o\", \"--output\", help=\"Output JSON file, e.g data.json\")\n",
        "\n",
        "    # parse passed arguments\n",
        "    args = parser.parse_args()\n",
        "    limit = args.limit\n",
        "    output = args.output\n",
        "    url = args.url\n",
        "\n",
        "    from pprint import pprint\n",
        "    for count, comment in enumerate(get_comments(url)):\n",
        "        if limit and count >= limit:\n",
        "            # break out of the loop when we exceed limit specified\n",
        "            break\n",
        "        if output:\n",
        "            # write comment as JSON to a file\n",
        "            with open(output, \"a\") as f:\n",
        "                # begin writing, adding an opening brackets\n",
        "                if count == 0:\n",
        "                    f.write(\"[\")\n",
        "                f.write(json.dumps(comment, ensure_ascii=False) + \",\")\n",
        "        else:\n",
        "            pprint(comment)\n",
        "            print(\"=\"*50)\n",
        "    print(\"total comments extracted:\", count)\n",
        "    if output:\n",
        "        # remove the last comma ','\n",
        "        with open(output, \"rb+\") as f:\n",
        "            f.seek(-1, os.SEEK_END)\n",
        "            f.truncate()\n",
        "        # add \"]\" to close the list in the end of the file\n",
        "        with open(output, \"a\") as f:\n",
        "            print(\"]\", file=f)"
      ],
      "metadata": {
        "id": "cnrAkAeETSAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a python program to access the search bar and search button on images.google.com and scrape 10\n",
        "images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
      ],
      "metadata": {
        "id": "TbdLbKJUTXAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_image_urls(\n",
        "    query: str,\n",
        "    max_links_to_fetch: int,\n",
        "    wd: webdriver,\n",
        "    sleep_between_interactions: int = 1,\n",
        "):\n",
        "    def scroll_to_end(wd):\n",
        "        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(sleep_between_interactions)\n",
        "\n",
        "    # Build the Google Query.\n",
        "    search_url = \"https://www.google.com/search?safe=off&site=&tbm=isch&source=hp&q={q}&oq={q}&gs_l=img\"\n",
        "\n",
        "    # load the page\n",
        "    wd.get(search_url.format(q=query))\n",
        "\n",
        "    # Declared as a set, to prevent duplicates.\n",
        "    image_urls = set()\n",
        "    image_count = 0\n",
        "    results_start = 0\n",
        "    while image_count < max_links_to_fetch:\n",
        "        scroll_to_end(wd)\n",
        "\n",
        "        # Get all image thumbnail results\n",
        "        thumbnail_results = wd.find_elements_by_css_selector(\"img.Q4LuWd\")\n",
        "        number_results = len(thumbnail_results)\n",
        "\n",
        "        print(\n",
        "            f\"Found: {number_results} search results. Extracting links from {results_start}:{number_results}\"\n",
        "        )\n",
        "\n",
        "        # Loop through image thumbnail identified\n",
        "        for img in thumbnail_results[results_start:number_results]:\n",
        "            # Try to click every thumbnail such that we can get the real image behind it.\n",
        "            try:\n",
        "                img.click()\n",
        "                time.sleep(sleep_between_interactions)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            # Extract image urls\n",
        "            actual_images = wd.find_elements_by_css_selector(\"img.n3VNCb\")\n",
        "            for actual_image in actual_images:\n",
        "                if actual_image.get_attribute(\n",
        "                    \"src\"\n",
        "                ) and \"http\" in actual_image.get_attribute(\"src\"):\n",
        "                    image_urls.add(actual_image.get_attribute(\"src\"))\n",
        "\n",
        "            image_count = len(image_urls)\n",
        "\n",
        "            # If the number images found exceeds our `num_of_images`, end the seaerch.\n",
        "            if len(image_urls) >= max_links_to_fetch:\n",
        "                print(f\"Found: {len(image_urls)} image links, done!\")\n",
        "                break\n",
        "        else:\n",
        "            # If we haven't found all the images we want, let's look for more.\n",
        "            print(\"Found:\", len(image_urls), \"image links, looking for more ...\")\n",
        "            time.sleep(SLEEP_BEFORE_MORE)\n",
        "\n",
        "            # Check for button signifying no more images.\n",
        "            not_what_you_want_button = \"\"\n",
        "            try:\n",
        "                not_what_you_want_button = wd.find_element_by_css_selector(\".r0zKGf\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            # If there are no more images return.\n",
        "            if not_what_you_want_button:\n",
        "                print(\"No more images available.\")\n",
        "                return image_urls\n",
        "\n",
        "            # If there is a \"Load More\" button, click it.\n",
        "            load_more_button = wd.find_element_by_css_selector(\".mye4qd\")\n",
        "            if load_more_button and not not_what_you_want_button:\n",
        "                wd.execute_script(\"document.querySelector('.mye4qd').click();\")\n",
        "\n",
        "        # Move the result startpoint further down.\n",
        "        results_start = len(thumbnail_results)\n",
        "\n",
        "    return image_urls\n",
        "\n",
        "\n",
        "def persist_image(folder_path: str, url: str):\n",
        "    try:\n",
        "        print(\"Getting image\")\n",
        "        # Download the image.  If timeout is exceeded, throw an error.\n",
        "        with timeout(GET_IMAGE_TIMEOUT):\n",
        "            image_content = requests.get(url).content\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR - Could not download {url} - {e}\")\n",
        "\n",
        "    try:\n",
        "        # Convert the image into a bit stream, then save it.\n",
        "        image_file = io.BytesIO(image_content)\n",
        "        image = Image.open(image_file).convert(\"RGB\")\n",
        "        # Create a unique filepath from the contents of the image.\n",
        "        file_path = os.path.join(\n",
        "            folder_path, hashlib.sha1(image_content).hexdigest()[:10] + \".jpg\"\n",
        "        )\n",
        "        with open(file_path, \"wb\") as f:\n",
        "            image.save(f, \"JPEG\", quality=IMAGE_QUALITY)\n",
        "        print(f\"SUCCESS - saved {url} - as {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR - Could not save {url} - {e}\")\n",
        "\n",
        "def search_and_download(search_term: str, target_path=\"./images\", number_images=5):\n",
        "    # Create a folder name.\n",
        "    target_folder = os.path.join(target_path, \"_\".join(search_term.lower().split(\" \")))\n",
        "\n",
        "    # Create image folder if needed.\n",
        "    if not os.path.exists(target_folder):\n",
        "        os.makedirs(target_folder)\n",
        "\n",
        "    # Open Chrome\n",
        "    with webdriver.Chrome() as wd:\n",
        "        # Search for images URLs.\n",
        "        res = fetch_image_urls(\n",
        "            search_term,\n",
        "            number_images,\n",
        "            wd=wd,\n",
        "            sleep_between_interactions=SLEEP_BETWEEN_INTERACTIONS,\n",
        "        )\n",
        "\n",
        "        # Download the images.\n",
        "        if res is not None:\n",
        "            for elem in res:\n",
        "                persist_image(target_folder, elem)\n",
        "        else:\n",
        "            print(f\"Failed to return links for term: {search_term}\")\n",
        "\n",
        "# Loop through all the search terms.\n",
        "for term in search_terms:\n",
        "    search_and_download(term, output_path, number_of_images)"
      ],
      "metadata": {
        "id": "TZrmG2FBTX63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a python program which searches all the product under a particular product from www.amazon.in. The\n",
        "product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for\n",
        "guitars."
      ],
      "metadata": {
        "id": "dUvjCX6UT27t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "# Function to extract Product Title\n",
        "def get_title(soup):\n",
        "\n",
        "\ttry:\n",
        "\t\t# Outer Tag Object\n",
        "\t\ttitle = soup.find(\"span\", attrs={\"id\":'productTitle'})\n",
        "\n",
        "\t\t# Inner NavigableString Object\n",
        "\t\ttitle_value = title.string\n",
        "\n",
        "\t\t# Title as a string value\n",
        "\t\ttitle_string = title_value.strip()\n",
        "\n",
        "\t\t# # Printing types of values for efficient understanding\n",
        "\t\t# print(type(title))\n",
        "\t\t# print(type(title_value))\n",
        "\t\t# print(type(title_string))\n",
        "\t\t# print()\n",
        "\n",
        "\texcept AttributeError:\n",
        "\t\ttitle_string = \"\"\n",
        "\n",
        "\treturn title_string\n",
        "\n",
        "# Function to extract Product Price\n",
        "def get_price(soup):\n",
        "\n",
        "\ttry:\n",
        "\t\tprice = soup.find(\"span\", attrs={'id':'priceblock_ourprice'}).string.strip()\n",
        "\n",
        "\texcept AttributeError:\n",
        "\t\tprice = \"\"\n",
        "\n",
        "\treturn price\n",
        "\n",
        "# Function to extract Product Rating\n",
        "def get_rating(soup):\n",
        "\n",
        "\ttry:\n",
        "\t\trating = soup.find(\"i\", attrs={'class':'a-icon a-icon-star a-star-4-5'}).string.strip()\n",
        "\n",
        "\texcept AttributeError:\n",
        "\n",
        "\t\ttry:\n",
        "\t\t\trating = soup.find(\"span\", attrs={'class':'a-icon-alt'}).string.strip()\n",
        "\t\texcept:\n",
        "\t\t\trating = \"\"\n",
        "\n",
        "\treturn rating\n",
        "\n",
        "# Function to extract Number of User Reviews\n",
        "def get_review_count(soup):\n",
        "\ttry:\n",
        "\t\treview_count = soup.find(\"span\", attrs={'id':'acrCustomerReviewText'}).string.strip()\n",
        "\n",
        "\texcept AttributeError:\n",
        "\t\treview_count = \"\"\n",
        "\n",
        "\treturn review_count\n",
        "\n",
        "# Function to extract Availability Status\n",
        "def get_availability(soup):\n",
        "\ttry:\n",
        "\t\tavailable = soup.find(\"div\", attrs={'id':'availability'})\n",
        "\t\tavailable = available.find(\"span\").string.strip()\n",
        "\n",
        "\texcept AttributeError:\n",
        "\t\tavailable = \"\"\n",
        "\n",
        "\treturn available\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\t# Headers for request\n",
        "\tHEADERS = ({'User-Agent':\n",
        "\t            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
        "\t            'Accept-Language': 'en-US, en;q=0.5'})\n",
        "\n",
        "\t# The webpage URL\n",
        "\tURL = \"https://www.amazon.com/Sony-PlayStation-Pro-1TB-Console-4/dp/B07K14XKZH/\"\n",
        "\n",
        "\t# HTTP Request\n",
        "\twebpage = requests.get(URL, headers=HEADERS)\n",
        "\n",
        "\t# Soup Object containing all data\n",
        "\tsoup = BeautifulSoup(webpage.content, \"lxml\")\n",
        "\n",
        "\t# Function calls to display all necessary product information\n",
        "\tprint(\"Product Title =\", get_title(soup))\n",
        "\tprint(\"Product Price =\", get_price(soup))\n",
        "\tprint(\"Product Rating =\", get_rating(soup))\n",
        "\tprint(\"Number of Product Reviews =\", get_review_count(soup))\n",
        "\tprint(\"Availability =\", get_availability(soup))\n",
        "\tprint()\n",
        "\tprint()"
      ],
      "metadata": {
        "id": "czB91JNwUVrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in\n",
        "“London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall\n",
        "reviews, privates from price, dorms from price, facilities and property description."
      ],
      "metadata": {
        "id": "TVaaOUoFUYSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "l=list()\n",
        "g=list()\n",
        "o={}\n",
        "k={}\n",
        "fac=[]\n",
        "fac_arr=[]\n",
        "headers={\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\"}\n",
        "\n",
        "target_url = \"https://www.booking.com/hotel/us/the-lenox.html?checkin=2022-12-28&checkout=2022-12-29&group_adults=2&group_children=0&no_rooms=1&selected_currency=USD\"\n",
        "\n",
        "resp = requests.get(target_url, headers=headers)\n",
        "\n",
        "soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "\n",
        "o[\"name\"]=soup.find(\"h2\",{\"class\":\"pp-header__title\"}).text\n",
        "o[\"address\"]=soup.find(\"span\",{\"class\":\"hp_address_subtitle\"}).text.strip(\"\\n\")\n",
        "o[\"rating\"]=soup.find(\"div\",{\"class\":\"d10a6220b4\"}).text\n",
        "\n",
        "fac=soup.find_all(\"div\",{\"class\":\"important_facility\"})\n",
        "for i in range(0,len(fac)):\n",
        "    fac_arr.append(fac[i].text.strip(\"\\n\"))\n",
        "\n",
        "\n",
        "ids= list()\n",
        "\n",
        "targetId=list()\n",
        "try:\n",
        "    tr = soup.find_all(\"tr\")\n",
        "except:\n",
        "    tr = None\n",
        "\n",
        "for y in range(0,len(tr)):\n",
        "    try:\n",
        "        id = tr[y].get('data-block-id')\n",
        "\n",
        "    except:\n",
        "        id = None\n",
        "\n",
        "    if( id is not None):\n",
        "        ids.append(id)\n",
        "\n",
        "print(\"ids are \",len(ids))\n",
        "\n",
        "\n",
        "for i in range(0,len(ids)):\n",
        "\n",
        "    try:\n",
        "        allData = soup.find(\"tr\",{\"data-block-id\":ids[i]})\n",
        "        try:\n",
        "            rooms = allData.find(\"span\",{\"class\":\"hprt-roomtype-icon-link\"})\n",
        "        except:\n",
        "            rooms=None\n",
        "\n",
        "\n",
        "        if(rooms is not None):\n",
        "            last_room = rooms.text.replace(\"\\n\",\"\")\n",
        "        try:\n",
        "            k[\"room\"]=rooms.text.replace(\"\\n\",\"\")\n",
        "        except:\n",
        "            k[\"room\"]=last_room\n",
        "\n",
        "        price = allData.find(\"div\",{\"class\":\"bui-price-display__value prco-text-nowrap-helper prco-inline-block-maker-helper prco-f-font-heading\"})\n",
        "        k[\"price\"]=price.text.replace(\"\\n\",\"\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        g.append(k)\n",
        "        k={}\n",
        "\n",
        "    except:\n",
        "        k[\"room\"]=None\n",
        "        k[\"price\"]=None\n",
        "\n",
        "\n",
        "l.append(g)\n",
        "l.append(o)\n",
        "l.append(fac_arr)\n",
        "print(l)"
      ],
      "metadata": {
        "id": "ovwOoKA6UuLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gjuiY19VU7HA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}